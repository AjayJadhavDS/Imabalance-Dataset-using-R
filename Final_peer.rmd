---
title: "Peer Assessment II"
output:
  html_document:
    pandoc_args: --number-sections
  pdf_document: default
---

<!--begin.rcode fakeFunction
# do something with myData, assume it is defined!
end.rcode-->

```{r libraries.etc, echo=FALSE, results='hide', message=FALSE}
cachedata = TRUE
cachemodels = TRUE
```

# Background

As a statistical consultant working for a real estate investment firm, your task is to develop a model to predict the selling price of a given home in Ames, Iowa. Your employer hopes to use this information to help assess whether the asking price of a house is higher or lower than the true value of the house. If the home is undervalued, it may be a good investment for the firm.

# Training Data and relevant packages

In order to better assess the quality of the model you will produce, the data have been randomly divided into three separate pieces: a training data set, a testing data set, and a validation data set. For now we will load the training data set, the others will be loaded and used later.


We are loading train data and filtering houses. I will only include houses that are sold under Normal sale Condition. Abnormal house sale can cause that price cant be predicted good. 


Use the code block below to load any necessary packages

```{r results='hide', message=FALSE, warning=FALSE}
library(statsr)
library(dplyr)
library(BAS)
library(DT)
library(pander)
library(tidyverse)
library(forecast)
library(ggthemes)
memory.limit(size=560000)
```


```{r load, message = FALSE}
load("ames_train.Rdata")
```

```{r someVar, echo=FALSE}
ames_train = sample_n(ames_train,500)
```

## Part 1 - Exploratory Data Analysis (EDA)

When you first get your data, it's very tempting to immediately begin fitting models and assessing how they perform.  However, before you begin modeling, it's absolutely essential to explore the structure of the data and the relationships between the variables in the data set.

Do a detailed EDA of the ames_train data set, to learn about the structure of the data and the relationships between the variables in the data set (refer to Introduction to Probability and Data, Week 2, for a reminder about EDA if needed). Your EDA should involve creating and reviewing many plots/graphs and considering the patterns and relationships you see. 

After you have explored completely, submit the three graphs/plots that you found most informative during your EDA process, and briefly explain what you learned from each (why you found each informative).

* * *
Lets have a look at our response variable *price* first:


```{r creategraphs}
ames_train = ames_train %>% filter(Sale.Condition == "Normal")
ames_train %>% 
    summarize(Q1 = quantile(price, 0.25), MEAN = mean(price), MEDIAN = median(price),Q3 = quantile(price, 0.75), IQR = IQR(price), STDEV = sd(price)) %>%
    mutate(SKEW = ifelse(MEAN > MEDIAN, "RIGHT", "LEFT"))%>%pandoc.table()
```

As we can see data is skewed to the right. 

Take a look at histogram log log(price) variable:
```{r}
p<-ggplot(aes(x = log(price) ) , data = ames_train) + 
geom_histogram(aes(fill=(Sale.Condition=='Normal') )) +
stat_bin( geom="text", aes(label=..count..) ,vjust = 0,hjust=0) +	
xlab('Natural log of Price (in US Dollars)')+
geom_vline(data=ames_train,xintercept=median(log(ames_train$price)), color="red")+
geom_vline(data=ames_train,xintercept=mean(log(ames_train$price)), color="blue")+
 ggtitle("Fig I. Natural Log of Price Distribution of Real Estate for Ames Iowa")
p
```

```{r}
par(mfrow=c(2,2))
qqnorm(ames_train$price, lty = 2)
qqline(ames_train$price)
plot(density(ames_train$price), main="Probability Density of Std. Residuals (price)", 
    xlab="Price", ylab="P(Price)")

qqnorm(log(ames_train$price), lty = 2)
qqline(log(ames_train$price))
plot(density(log(ames_train$price)), main="Probability Density of Std. Residuals (log_price)", 
    xlab="Log Price", ylab="P(Log Price)")
```


Here we see that data QQ plot of *log(price)* is more linear than QQ plot of *price* and that appears more normally distributed. 

Now lets explore relationships between price and other variables.

**Price vs quality
```{r}
ames_train %>% 
    group_by(Overall.Qual) %>% 
    summarize(Q1 = quantile(price, 0.25), MEAN = mean(price), MEDIAN = median(price),Q3 = quantile(price, 0.75), IQR = IQR(price), STDEV = sd(price)) %>%
    mutate(SKEW = ifelse(MEAN > MEDIAN, "RIGHT", "LEFT")) %>%
    pandoc.table
median_data = ames_train %>% 
    group_by(Overall.Qual) %>% 
    summarize(med_price = median(price), IQR_price = IQR(price))

cc = sample(colorspace::rainbow_hcl(27, c = 100, l=60,start = 0, end = 300), 10)
ames_train %>%
    left_join(median_data) %>%
    mutate(Overall.Qual = reorder(Overall.Qual, -med_price)) %>%
    ggplot(aes(x=Overall.Qual, y = price)) +
    geom_jitter(aes(color=Overall.Qual),alpha= 0.4, height = 0, width = 0.3) +
    geom_boxplot(fill=NA, outlier.shape=NA) +
    scale_color_manual(values = cc) +
    theme_solarized_2() +
    theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_text(angle = 90, hjust = 1)) +
    guides(fill=FALSE, color=FALSE) +
    labs(title = "Distribution of Home Prices by Overall Quality", 
                 y = "Home Price", 
                 x = "Overall Quality")
```


We can see that there is strong positive relationship between price and Quality of the houses. Few houses have extreme values and we can came to a conclusion that that values can cause that model behaves differently with those values.

**Area vs Price**
```{r}
par(mfrow=c(2,2))
qqnorm(ames_train$area, lty = 2)
qqline(ames_train$area)
plot(density(ames_train$area), main="Probability Density of Std. Residuals (area)", 
    xlab="Area", ylab="P(Area)")

qqnorm(log(ames_train$area), lty = 2)
qqline(log(ames_train$area))
plot(density(log(ames_train$area)), main="Probability Density of Std. Residuals (log(area))", 
    xlab="log(Area)", ylab="P(log(Area))")
library(gridExtra)
left = ggplot(ames_train, aes(x=area, y=log(price))) + 
        geom_point() + 
        geom_smooth(method=loess, fill="red", color="red") +
        geom_smooth(method=lm, fill="blue", color="blue") +
        ylim(9,14) +
        theme_solarized_2() +
        labs(title = "Log Price versus Area", 
                 y = "Log Price", 
                 x = "Area")

right = ggplot(ames_train, aes(x=log(area), y=log(price))) + 
        geom_point() + 
        geom_smooth(method=loess, fill="red", color="red") +
        geom_smooth(method=lm, fill="blue", color="blue") +
        ylim(9,14) +
        theme_solarized_2() +
        labs(title = "Log Price versus Log Area", 
                 y = "Log Price", 
                 x = "Log Area")

grid.arrange(left,right,nrow=1)
```


We see that if log transformed data appears to have normal distribution and also after transformation we notice that relationship is more linear.
* * *

## Part 2 - Development and assessment of an initial model, following a semi-guided process of analysis

### Section 2.1 An Initial Model
In building a model, it is often useful to start by creating a simple, intuitive initial model based on the results of the exploratory data analysis. (Note: The goal at this stage is **not** to identify the "best" possible model but rather to choose a reasonable and understandable starting point. Later you will expand and revise this model to create your final model.

Based on your EDA, select *at most* 10 predictor variables from “ames_train” and create a linear model for `price` (or a transformed version of price) using those variables. Provide the *R code* and the *summary output table* for your model, a *brief justification* for the variables you have chosen, and a *brief discussion* of the model results in context (focused on the variables that appear to be important predictors and how they relate to sales price).

* * *

As we see in the EDA part area variable is transformed and model is:

```{r fit_model}
model1 = lm(log(price) ~ Overall.Qual + Neighborhood + Exter.Qual + log(area) + Kitchen.Qual + Overall.Cond + log(Lot.Area) + Year.Built + Year.Remod.Add, ames_train)
summary(model1)
```


Variable explanation:


1. log(area):Size of house important to price prediction - Coefficient Estimate = ($0.4260321) - log chosen to make square footage less right skewed

2.log(Lot.Area) - Size of lot important to price prediction - - Coefficient Estimate = ($0.1439475) log chosen to make square footage less right skewed

3.Year.Built - Coefficient Estimate = ($0.0042068) House age can be important for many reasons, some like older house and some like newer

Adjusted R^2 is 0.9079.
* * *

### Section 2.2 Model Selection

Now either using `BAS` another step-wise selection procedure choose the "best" model you can, using your initial model as your starting point. Try at least two different model selection methods and compare their results. Do they both arrive at the same model or do they disagree? What do you think this means?

* * *

The BAS package and both BIC and AIC step selection were run using the initial model *model1*

```{r model_select}
print("The AIC step fit process:")
fit_AIC = step(model1)
print("The BIC step fit process:")
fit_BIC = step(model1, k=log(nrow(ames_train)))
ames0.bas =  bas.lm(log(price) ~ Overall.Qual + Neighborhood + Exter.Qual + log(area) + Kitchen.Qual + Overall.Cond + log(Lot.Area) + Year.Built + Year.Remod.Add, 
                   data=ames_train,
                  initprobs = "eplogp",
                   prior="BIC",
                   modelprior=uniform()) 

coefs <- coef(ames0.bas, estimator = "BMA")
# find posterior probabilities 
coefs_bas <- data.frame(parameter = coefs$namesx, post_mean = coefs$postmean, post_SD = coefs$postsd, post_pne0 = coefs$probne0) %>% arrange(post_pne0) %>% filter(parameter != "Intercept")
coefs_bas$parameter <- factor(coefs_bas$parameter, levels = coefs_bas$parameter[order(coefs_bas$post_pne0, decreasing = TRUE)])
high_pne0 <- data.frame(parameter = coefs_bas$parameter, post_pne0 = coefs_bas$post_pne0) %>% filter(post_pne0 > 0.5)
# Plot the Data
print("Results from the BAS fit:")
ggplot(coefs_bas, aes(x = parameter, y = post_pne0)) + 
    geom_pointrange(aes(ymax = post_pne0), ymin = 0) +
    geom_pointrange(data=high_pne0, aes(x = parameter, y = post_pne0, ymax = post_pne0), ymin = 0, color = "red") +
    geom_hline(yintercept = 0.5, color = "red") +
    labs(title = "Posterior Marginal Inclusion Probabilities of Explanatory Variables",x="Explanatory Variable",y = "Marginal Inclusion Probability") +
    theme(axis.text.x = element_text(angle = 60, hjust = 1), plot.title = element_text(hjust = 0.5))
```


 Bayes Model Average (BMA) and the step AIC models are using the same variables from initial model.  The step BIC model doesn't include *Year.Remod.Add* variable. I will be using  BMA model.

* * *

### Section 2.3 Initial Model Residuals
One way to assess the performance of a model is to examine the model's residuals. In the space below, create a residual plot for your preferred model from above and use it to assess whether your model appears to fit the data well. Comment on any interesting structure in the residual plot (trend, outliers, etc.) and briefly discuss potential implications it may have for your model and inference / prediction you might produce.

* * *



```{r model_resid}
pred_train <- predict(ames0.bas,ames_train,estimator = "BMA")
resid_train <- na.omit(log(ames_train$price) - pred_train$fit)
plot_dat <- data.frame(fitted = na.omit(pred_train$fit), resid = resid_train)
ggplot(plot_dat, aes(x = fitted, y = resid)) + geom_point(pch=21, fill=NA) + 
    geom_smooth(color= "red", se = FALSE, lwd = 0.5) + 
    labs(title = "Residuals vs. Fitted Plot", y = "Residuals", x = "Fitted values") +
    theme(plot.title = element_text(hjust = 0.5)) +
    theme_solarized_2()
```


As we can see residuals are around zero and they are normally distributed. I found that model is good for linear regression. 
* * *

### Section 2.4 Initial Model RMSE

You can calculate it directly based on the model output. Be specific about the units of your RMSE (depending on whether you transformed your response variable). The value you report will be more meaningful if it is in the original units (dollars).

* * *




```{r model_rmse}
rmse_train = sqrt(mean((na.omit(ames_train$price - exp(pred_train$fit)))^2))
rmse_train
```

We can see that the within-sample root-mean-squared error is 22206.45 dollars.

* * *

### Section 2.5 Overfitting 

The process of building a model generally involves starting with an initial model (as you have done above), identifying its shortcomings, and adapting the model accordingly. This process may be repeated several times until the model fits the data reasonably well. However, the model may do well on training data but perform poorly out-of-sample (meaning, on a data set other than the original training data) because the model is overly-tuned to specifically fit the training data. This is called “overfitting.” To determine whether overfitting is occurring on a model, compare the performance of a model on both in-sample and out-of-sample data sets. To look at performance of your initial model on out-of-sample data, you will use the data set `ames_test`.

```{r loadtest, message = FALSE}
load("ames_test.Rdata")
```

```{r someVar2, echo=FALSE}
ames_test = sample_n(ames_test,100)
```

Use your model from above to generate predictions for the housing prices in the test data set.  Are the predictions significantly more accurate (compared to the actual sales prices) for the training data than the test data?  Why or why not? Briefly explain how you determined that (what steps or processes did you use)?

* * *



```{r initmodel_test}
ames_test = ames_test %>% filter(Neighborhood != "Landmrk", Sale.Condition == "Normal")
pred_test = predict(ames0.bas,newdata=ames_test,estimator = "BMA")
resid_test = ames_test$price - exp(pred_test$fit)
rmse_test = sqrt(mean(resid_test^2))
rmse_test
```

We can see that the within-sample root-mean-squared error is 24450.34 dollars. There is no overfitting based on the test data. Difference is +2243.89 dollars.

* * *

**Note to the learner:** If in real-life practice this out-of-sample analysis shows evidence that the training data fits your model a lot better than the test data, it is probably a good idea to go back and revise the model (usually by simplifying the model) to reduce this overfitting. For simplicity, we do not ask you to do this on the assignment, however.

## Part 3 Development of a Final Model

Now that you have developed an initial model to use as a baseline, create a final model with *at most* 20 variables to predict housing prices in Ames, IA, selecting from the full array of variables in the data set and using any of the tools that we introduced in this specialization.  

Carefully document the process that you used to come up with your final model, so that you can answer the questions below.

### Section 3.1 Final Model

Provide the summary table for your model.

* * *

Model contains all 18 variables:


```{r model_playground}
ames.bas =  bas.lm(log(price) ~ Overall.Qual+Neighborhood+Exter.Qual+log(area)+Kitchen.Qual+X1st.Flr.SF+Total.Bsmt.SF+Year.Built+Year.Remod.Add+Garage.Cars+BsmtFin.SF.1+log(area):Overall.Qual:X1st.Flr.SF+Overall.Qual:X1st.Flr.SF+BsmtFin.SF.1:Overall.Qual:X1st.Flr.SF+log(area):Overall.Qual:Year.Built+log(area):Overall.Qual+Garage.Cars:Overall.Qual+log(area):Year.Built+log(area):Garage.Cars, 
                   data=ames_train,
                  initprobs = "eplogp",
                   prior="BIC",
                   modelprior=uniform()) 
```

* * *

### Section 3.2 Transformation

Did you decide to transform any variables?  Why or why not? Explain in a few sentences.

* * *

After performing EDA variable *area* and was transformed into *log(area)* because *log(area)* has better fit. Also variable. Also variable *Lot.Area* is log-transformed.


* * *

### Section 3.3 Variable Interaction

Did you decide to include any variable interactions? Why or why not? Explain in a few sentences.

* * *

The variables that contribute the most to the final Adjusted $R^2$are chosen from their F-Statistic value determined by an anova model analysis. The colinearity between these variables is investigated and it was determined that no such relationship exists. Most of the interaction terms were relationships between log(area) and the other terms. One such interaction was log(area):Overall.Qual, indicating that the interaction between house size and quality was important for predicting log(price).



* * *

### Section 3.4 Variable Selection

What method did you use to select the variables you included? Why did you select the method you used? Explain in a few sentences.

* * *

Variables in this model are the same variables that are used in the initial model.The BAS package using Bayes Model Averaging (BMA) was used to manage variable selection. The BMA process reduces or eliminates the coefficients that have a low posterior probability of inclusion in the model. This allows more information to be preserved by not totally eliminating some variables, but also limits the effects of overfitting by reducing the magnitude of the coefficients for low posterior probability variables.




* * *



* * *

## Part 4 Final Model Assessment

### Section 4.1 Final Model Residual

For your final model, create and briefly interpret an informative plot of the residuals.

* * *

```{r}
pred_train <- predict(ames.bas,ames_train,estimator = "BMA")
resid_train <- na.omit(log(ames_train$price) - pred_train$fit)
plot_dat <- data.frame(fitted = na.omit(pred_train$fit), resid = resid_train)
ggplot(plot_dat, aes(x = fitted, y = resid)) + 
    geom_point(pch=21, fill=NA) +
    geom_smooth(color= "red", se = FALSE, lwd = 0.5) + 
    labs(title = "Residuals vs. Fitted Plot", y = "Residuals", x = "Fitted values") +
    theme(plot.title = element_text(hjust = 0.5)) +
    theme_solarized_2()
mu_resid <- mean(resid_train, na.rm=TRUE)
sd_resid <- sd(resid_train, na.rm=TRUE)
std_resid <- (resid_train-mu_resid)/sd_resid
par(mfrow=c(1,2))
qqnorm(std_resid, lty = 2)
qqline(std_resid)
plot(density(std_resid), main="Probability Density of Std. Residuals", 
    xlab="Std. Residuals", ylab="P(Std. Residuals)")
```


Residuals are normally distributed.
Model has consistent bias and variance and it is good linear regression for the data.

* * *

### Section 4.2 Final Model RMSE

For your final model, calculate and briefly comment on the RMSE.

* * *

```{r}
rmse_train = sqrt(mean((na.omit(ames_train$price - exp(pred_train$fit)))^2))
rmse_train
mes_test = ames_test %>% filter(Neighborhood != "Landmrk", Sale.Condition == "Normal") 
pred_test = predict(ames.bas,newdata=ames_test,estimator = "BMA")
resid_test = ames_test$price - exp(pred_test$fit)
rmse_test = sqrt(mean(resid_test^2))
resid_test
```

We can see that error is reduced, both within-sample and out-of-sample RMSE values are lower.
* * *

### Section 4.3 Final Model Evaluation

What are some strengths and weaknesses of your model?

* * *

```{r}
dummy_error = ames_test$price - median(ames_train$price)
dummy_rmse = sqrt(mean(dummy_error^2))
dummy_rmse
```

We can expect that values are around +- 25000 dollars. Regressor has prediction error $76645.
The model gives consistent predictions for a majority of the houses and performs well for houses not present in the training data. Moreover, the median house price in the training data was $155,500, so this error is about ?13% of the typical home price.

* * *

### Section 4.4 Final Model Validation

Testing your final model on a separate, validation data set is a great way to determine how your model will perform in real-life practice. 

You will use the “ames_validation” data set to do some additional assessment of your final model. Discuss your findings, be sure to mention:
* What is the RMSE of your final model when applied to the validation data?  
* How does this value compare to that of the training data and/or testing data?
* What percentage of the 95% predictive confidence (or credible) intervals contain the true price of the house in the validation data set?  
* From this result, does your final model properly reflect uncertainty?

```{r loadvalidation, message = FALSE}
load("ames_validation.Rdata")
```

```{r someVar3, echo=FALSE}
ames_validation = sample_n(ames_validation,100)
```

* * *


```{r model_validate}
ames_validation = ames_validation %>% filter(Sale.Condition == "Normal") 
pred_valid_se = predict(ames.bas,newdata=ames_validation,estimator = "BMA", se.fit=TRUE)
resid_valid = ames_validation$price - exp(pred_valid_se$fit) 
rmse_valid = sqrt(mean(resid_valid^2))
rmse_valid
ci_audience <- confint(pred_valid_se, parm="pred") %>% exp 
cbind(select(ames_validation, price), ci_audience[,]) %>% mutate(inside = ifelse(price >= `2.5%` & price <= `97.5%`,TRUE,FALSE)) %>% summarize(mean(inside)) 
```


The model properly reflects the uncertainty in the predictions as about 5% of the time the actual house price is outside of the 95% credible interval for the predicted house price for that value.
* * *

## Part 5 Conclusion

Provide a brief summary of your results, and a brief discussion of what you have learned about the data and your model. 

* * *
There are few variables that can effect price of the houses in Iowa. We can see that overall quality and locations of the properties have large impact of the price. Some other variables are significant, such as Area, Lot.Area, basement size,kitchen and year that house was remodel. Linear models are used all the times to predict house market. But, this model exclude unusual house sales. Some other model is needed for this type of prediction.Model has prediction of +- 20 000$.


* * *
